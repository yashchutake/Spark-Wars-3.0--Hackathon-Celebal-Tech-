{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02c82ea8-d015-49a8-935b-9fa76658a406",
     "showTitle": true,
     "title": "DROP DATABASE IF EXISTS"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[87]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP DATABASE IF EXISTS bronze CASCADE \")\n",
    "spark.sql(\"DROP DATABASE IF EXISTS silver CASCADE \")\n",
    "spark.sql(\"DROP DATABASE IF EXISTS gold CASCADE \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2afb3f34-0ddc-48d7-a22d-2f4843fa9f12",
     "showTitle": true,
     "title": "Pre-Requisite : Create Schemas"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[88]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE bronze\")\n",
    "spark.sql(\"CREATE DATABASE silver\")\n",
    "spark.sql(\"CREATE DATABASE gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a46d115b-cda0-453e-a1b3-028c76489e41",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c87d382-4821-4d7b-a82e-57e8c874eaaf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Section A: Data Ingestion and Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75397143-10db-49cd-acff-a225384fe068",
     "showTitle": true,
     "title": "Section A: Data Ingestion and Bronze Layer"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[89]: [FileInfo(path='dbfs:/FileStore/tables/66a262dd3659f_customer_dataset.csv', name='66a262dd3659f_customer_dataset.csv', size=81951758, modificationTime=1722183027000),\n FileInfo(path='dbfs:/FileStore/tables/66a2634026e86_transactions_dataset.csv', name='66a2634026e86_transactions_dataset.csv', size=75105439, modificationTime=1722183054000)]"
     ]
    }
   ],
   "source": [
    "# dbutils.fs.ls('dbfs:/FileStore/tables')\n",
    "dbutils.fs.ls('dbfs:/FileStore/tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15330a29-9fab-48ae-a286-1a45a687f68a",
     "showTitle": true,
     "title": "Create a table named 'transactions_bronzeâ€™"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[90]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "# transactions_df=spark.read.csv()\n",
    "spark.sql(\"\"\"USE schema bronze\"\"\")\n",
    "spark.sql(\"\"\" CREATE TABLE IF NOT EXISTS transactions_bronze using CSV options (path=\"dbfs:/FileStore/tables/66a2634026e86_transactions_dataset.csv\",header=\"True\",mode=\"FAILFAST\",inferSchema \"True\")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c15a497e-3143-45c9-922c-1efa2d6821b8",
     "showTitle": true,
     "title": "Create a table named 'customer_bronze"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[91]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "# transactions_df=spark.read.csv()\n",
    "spark.sql(\"\"\"USE schema bronze\"\"\")\n",
    "spark.sql(\"\"\" CREATE TABLE IF NOT EXISTS customer_bronze using CSV options (path=\"dbfs:/FileStore/tables/66a262dd3659f_customer_dataset.csv\",header=\"True\",mode=\"FAILFAST\",inferSchema \"True\")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8be148d4-cbdf-44ec-9d0c-5760e46f1540",
     "showTitle": true,
     "title": "Preview and Count Records:bronze tables."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------------+-------------------+----------------+-----------------+------------------+\n|transaction_id|customer_id|transaction_amount|   transaction_date|transaction_type|merchant_category|transaction_status|\n+--------------+-----------+------------------+-------------------+----------------+-----------------+------------------+\n|             0| cust_02732|           1014.73|2023-01-01 00:00:00|        purchase|         clothing|         completed|\n|             1| cust_43567|           3138.73|2023-01-01 01:00:00|          refund|         clothing|            failed|\n|             2| cust_42613|           1647.88|2023-01-01 02:00:00|          refund|           travel|         completed|\n|             3| cust_45891|           4864.71|2023-01-01 03:00:00|        purchase|         clothing|         completed|\n|             4| cust_21243|           3623.23|2023-01-01 04:00:00|        purchase|         clothing|            failed|\n+--------------+-----------+------------------+-------------------+----------------+-----------------+------------------+\n\n+-----------+---+-----------+--------------+---------+--------+----+------------+------------------+-----------------------+-------------------------+\n|customer_id|age|        job|marital_status|education| balance|loan|contact_type|last_contact_month|days_since_last_contact|previous_campaign_outcome|\n+-----------+---+-----------+--------------+---------+--------+----+------------+------------------+-----------------------+-------------------------+\n| cust_00000| 62|blue-collar|       married| tertiary|15148.37| yes|   telephone|           October|                     82|                  unknown|\n| cust_00001| 65| management|        single|secondary|15016.34|  no|   telephone|              June|                      8|                  failure|\n| cust_00002| 71|   services|       married|secondary| 7151.54| yes|    cellular|           January|                     35|                  unknown|\n| cust_00003| 18| technician|        single| tertiary| 9154.57| yes|    cellular|            August|                     27|                  failure|\n| cust_00004| 21|    student|       married| tertiary| 3984.89| yes|   telephone|              July|                     52|                  success|\n+-----------+---+-----------+--------------+---------+--------+----+------------+------------------+-----------------------+-------------------------+\n\n+--------+\n|count(1)|\n+--------+\n| 1000000|\n+--------+\n\n+--------+\n|count(1)|\n+--------+\n| 1000000|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Preview first 5 rows\n",
    "#Display the first 5 rows of both transactions_bronze and customer_bronze tables.\n",
    "spark.sql(\"SELECT * FROM bronze.transactions_bronze LIMIT 5\").show()\n",
    "spark.sql(\"SELECT * FROM bronze.customer_bronze LIMIT 5\").show()\n",
    "\n",
    "# Count total records\n",
    "transactions_count = spark.sql(\"SELECT COUNT(*) FROM bronze.transactions_bronze\").show()\n",
    "customers_count = spark.sql(\"SELECT COUNT(*) FROM bronze.customer_bronze\").show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d43e94fc-0ba0-4af6-adf9-f5a23a781b12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "345dfcbb-1670-431f-a5d0-8886bd86e92e",
     "showTitle": true,
     "title": "Questions:   1.1 Extract records where balance is greater than 15000 and loan is 'yes' from customers_bronze."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------------+--------------+---------+--------+----+------------+------------------+-----------------------+-------------------------+\n|customer_id|age|         job|marital_status|education| balance|loan|contact_type|last_contact_month|days_since_last_contact|previous_campaign_outcome|\n+-----------+---+------------+--------------+---------+--------+----+------------+------------------+-----------------------+-------------------------+\n| cust_00000| 62| blue-collar|       married| tertiary|15148.37| yes|   telephone|           October|                     82|                  unknown|\n| cust_00006| 21|     student|      divorced|secondary|19600.01| yes|   telephone|           October|                     94|                  failure|\n| cust_00011| 68|  technician|        single| tertiary|17575.37| yes|   telephone|               May|                     41|                  success|\n| cust_00020| 56|  technician|       married|  primary|15840.76| yes|   telephone|         September|                     47|                  unknown|\n| cust_00028| 31|     student|        single| tertiary| 19653.6| yes|   telephone|               May|                     85|                  success|\n| cust_00048| 57|     student|      divorced|secondary| 15315.1| yes|   telephone|          November|                     65|                  failure|\n| cust_00050| 19|    services|        single|secondary|19531.05| yes|    cellular|               May|                     86|                  failure|\n| cust_00054| 49|  management|        single|secondary|15099.78| yes|    cellular|              July|                     28|                  success|\n| cust_00073| 33|  management|      divorced| tertiary|16940.98| yes|    cellular|            August|                     69|                  success|\n| cust_00076| 60| blue-collar|        single|  primary|17577.59| yes|   telephone|           October|                     94|                  failure|\n| cust_00081| 19|  management|        single|secondary|19587.35| yes|   telephone|          December|                     21|                  success|\n| cust_00155| 53|    services|      divorced|secondary|18199.48| yes|   telephone|              June|                     77|                  success|\n| cust_00157| 77|entrepreneur|      divorced|secondary|16981.44| yes|   telephone|           October|                     37|                  failure|\n| cust_00170| 54| blue-collar|       married|  primary|15242.55| yes|   telephone|             April|                     57|                  failure|\n| cust_00180| 72|  management|      divorced|secondary|19186.18| yes|    cellular|             March|                     12|                  failure|\n| cust_00182| 56| blue-collar|        single|secondary|17285.96| yes|    cellular|           October|                     91|                  unknown|\n| cust_00187| 74|    services|      divorced|secondary| 19226.1| yes|    cellular|          November|                     41|                  success|\n| cust_00189| 58|entrepreneur|       married| tertiary|18195.87| yes|   telephone|               May|                     60|                  unknown|\n| cust_00197| 52|  management|       married| tertiary|16935.62| yes|   telephone|           October|                     19|                  success|\n| cust_00206| 44|     student|      divorced| tertiary|15409.03| yes|    cellular|             March|                     78|                  failure|\n+-----------+---+------------+--------------+---------+--------+----+------------+------------------+-----------------------+-------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Extract records where balance is greater than 15000 and loan is 'yes' from customers_bronze.\n",
    "customers_filtered = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM bronze.customer_bronze\n",
    "WHERE balance > 15000 AND lower(trim(loan)) = 'yes'\n",
    "\"\"\")\n",
    "customers_filtered.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecdfdb97-2618-43d6-b806-66fe38d36348",
     "showTitle": true,
     "title": "1.2 Extract transactions from transactions_bronze where transaction_type is 'purchase' and merchant_category is 'travel'."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------------+-------------------+----------------+-----------------+------------------+\n|transaction_id|customer_id|transaction_amount|   transaction_date|transaction_type|merchant_category|transaction_status|\n+--------------+-----------+------------------+-------------------+----------------+-----------------+------------------+\n|             9| cust_46884|            195.86|2023-01-01 09:00:00|        purchase|           travel|         completed|\n|            13| cust_39512|           2514.84|2023-01-01 13:00:00|        purchase|           travel|         completed|\n|            28| cust_37619|           4956.73|2023-01-02 04:00:00|        purchase|           travel|            failed|\n|            31| cust_01871|           4019.53|2023-01-02 07:00:00|        purchase|           travel|            failed|\n|            45| cust_19129|           2460.44|2023-01-02 21:00:00|        purchase|           travel|            failed|\n|            47| cust_49866|           2579.99|2023-01-02 23:00:00|        purchase|           travel|         completed|\n|            51| cust_43890|            4455.6|2023-01-03 03:00:00|        purchase|           travel|            failed|\n|            53| cust_43136|           4984.55|2023-01-03 05:00:00|        purchase|           travel|            failed|\n|            54| cust_26752|           2366.38|2023-01-03 06:00:00|        purchase|           travel|            failed|\n|            55| cust_23605|           2137.02|2023-01-03 07:00:00|        purchase|           travel|         completed|\n|            56| cust_06021|            1002.2|2023-01-03 08:00:00|        purchase|           travel|            failed|\n|            70| cust_47883|            2659.5|2023-01-03 22:00:00|        purchase|           travel|         completed|\n|            77| cust_12372|           4069.86|2023-01-04 05:00:00|        purchase|           travel|         completed|\n|            79| cust_45830|           2167.98|2023-01-04 07:00:00|        purchase|           travel|            failed|\n|            80| cust_39876|           2413.33|2023-01-04 08:00:00|        purchase|           travel|            failed|\n|            83| cust_38019|           4495.39|2023-01-04 11:00:00|        purchase|           travel|            failed|\n|            87| cust_03918|           2577.12|2023-01-04 15:00:00|        purchase|           travel|         completed|\n|            88| cust_09359|           4830.29|2023-01-04 16:00:00|        purchase|           travel|         completed|\n|            90| cust_23482|           1316.19|2023-01-04 18:00:00|        purchase|           travel|            failed|\n|            93| cust_35725|           3368.81|2023-01-04 21:00:00|        purchase|           travel|            failed|\n+--------------+-----------+------------------+-------------------+----------------+-----------------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1.2 Extract transactions from transactions_bronze where transaction_type is 'purchase' and merchant_category is 'travel'.\n",
    "transactions_filtered = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM bronze.transactions_bronze\n",
    "WHERE transaction_type = 'purchase' AND merchant_category = 'travel'\n",
    "\"\"\")\n",
    "transactions_filtered.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a4ca4a-3de6-42f2-a5ae-4d9ff7127716",
     "showTitle": true,
     "title": "1.3 Find the job type of the customer who have done max amount of transaction"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|       job|\n+----------+\n|management|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1.3 Find the job type of the customer who have done max amount of transaction \n",
    "max_transaction = spark.sql(\"\"\"\n",
    "SELECT customer_id, SUM(transaction_amount) as total_amount\n",
    "FROM bronze.transactions_bronze\n",
    "GROUP BY customer_id\n",
    "ORDER BY total_amount DESC\n",
    "LIMIT 1\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "max_transaction_customer_id = max_transaction['customer_id']\n",
    "\n",
    "customer_job = spark.sql(f\"\"\"\n",
    "SELECT job\n",
    "FROM bronze.customer_bronze\n",
    "WHERE customer_id = '{max_transaction_customer_id}'\n",
    "\"\"\")\n",
    "customer_job.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82fd3916-511f-40c3-8be7-e046e89dc654",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Section B: Create Silver Tables with Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd63fe9-d400-434c-a036-0c553f5f133d",
     "showTitle": true,
     "title": "Section B: Create Silver Tables with Transformations"
    }
   },
   "outputs": [],
   "source": [
    "# Load bronze tables\n",
    "transactions_bronze_df = spark.table(\"bronze.transactions_bronze\")\n",
    "customers_bronze_df = spark.table(\"bronze.customer_bronze\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e75bc48-b692-4ef2-94bf-c9f949737d79",
     "showTitle": true,
     "title": "Clean transactions_bronze table"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[97]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "# Clean transactions_bronze table\n",
    "transactions_silver_df = transactions_bronze_df.dropna(subset=[\"customer_id\"]).dropDuplicates()\n",
    "transactions_silver_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/dataset/transactions_silver_delta\")\n",
    "spark.sql(\"\"\"\n",
    "    drop table if exists silver.transactions_silver\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE silver.transactions_silver\n",
    "    USING DELTA\n",
    "    LOCATION '/mnt/dataset/transactions_silver_delta'\n",
    "\"\"\")\n",
    "\n",
    "# Clean customer_bronze table\n",
    "customers_silver_df = customers_bronze_df.dropDuplicates()\n",
    "customers_silver_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/dataset/customers_silver_delta\")\n",
    "spark.sql(\"\"\"\n",
    "    drop table if exists silver.customers_silver\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE silver.customers_silver\n",
    "    USING DELTA\n",
    "    LOCATION '/mnt/dataset/customers_silver_delta'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64162c24-4c41-4cb8-b5a3-a1c25474084d",
     "showTitle": true,
     "title": "Optimization Techniques:"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# Convert the timestamp string to timestamp type if needed\n",
    "transactions_silver_df = transactions_silver_df.withColumn(\"timestamp_col\", transactions_silver_df[\"transaction_date\"].cast(\"timestamp\"))  \n",
    "\n",
    "# Extract the date from the timestamp\n",
    "transactions_silver_df = transactions_silver_df.withColumn(\"date_col\", date_format(transactions_silver_df[\"timestamp_col\"], \"yyyy-MM-dd\"))\n",
    "\n",
    "# Show the result\n",
    "# transactions_silver_df.select(\"transaction_date\",\"timestamp_col\",\"date_col\").show()\n",
    "# transactions_silver_df[\"date_col\"].distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53af15f1-515e-4ca2-9ed4-4deeeb552fda",
     "showTitle": true,
     "title": "Optimization Techniques:"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[113]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "# Optimization Techniques:\n",
    "# Apply partitioning on the transactions_silver table by transaction_date. \n",
    "transactions_silver_df.write.partitionBy(\"transaction_type\").mode(\"overwrite\").format(\"delta\").save(\"/mnt/dataset/transactions_silver_partitioned_delta\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE silver.transactions_silver\n",
    "    USING DELTA\n",
    "    LOCATION '/mnt/dataset/transactions_silver_partitioned_delta'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1e4be30-c05a-4b31-91dd-e7e65d46d88d",
     "showTitle": true,
     "title": "Apply bucketing on the bank_marketing_silver table by customer_id."
    }
   },
   "outputs": [],
   "source": [
    "# Apply bucketing on the bank_marketing_silver table by customer_id.\n",
    "customers_silver_df.write.bucketBy(8, \"customer_id\").mode(\"overwrite\").format(\"parquet\").saveAsTable(\"silver.customers_silver_bucketed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "502ff731-581c-4673-9521-2c27094deece",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72b4882a-d3da-4f72-95b4-0e25d6b7e13c",
     "showTitle": true,
     "title": "2.1 Calculate the cumulative transaction amount for each merchant_category ordered by transaction_date available in transactions_silver."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------------------+\n|merchant_category|   transaction_date| cumulative_amount|\n+-----------------+-------------------+------------------+\n|           travel|2023-01-01 02:00:00|           1647.88|\n|           travel|2023-01-01 09:00:00|1843.7400000000002|\n|           travel|2023-01-01 13:00:00|           4358.58|\n|           travel|2023-01-01 21:00:00|           5516.74|\n|           travel|2023-01-01 22:00:00| 7683.049999999999|\n|           travel|2023-01-01 23:00:00|          10263.43|\n|           travel|2023-01-02 03:00:00|11653.810000000001|\n|           travel|2023-01-02 04:00:00|          16610.54|\n|           travel|2023-01-02 05:00:00|          21604.33|\n|           travel|2023-01-02 07:00:00|          25623.86|\n|           travel|2023-01-02 08:00:00|          29675.96|\n|           travel|2023-01-02 15:00:00|          31186.64|\n|           travel|2023-01-02 16:00:00|          35223.04|\n|           travel|2023-01-02 21:00:00|          37683.48|\n|           travel|2023-01-02 23:00:00|          40263.47|\n|           travel|2023-01-03 03:00:00|          44719.07|\n|           travel|2023-01-03 05:00:00|          49703.62|\n|           travel|2023-01-03 06:00:00|           52070.0|\n|           travel|2023-01-03 07:00:00|          54207.02|\n|           travel|2023-01-03 08:00:00|55209.219999999994|\n+-----------------+-------------------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Calculate the cumulative transaction amount for each merchant_category ordered by transaction_date available in transactions_silver. \n",
    "spark.sql(\"\"\"\n",
    "SELECT merchant_category, transaction_date, SUM(transaction_amount) OVER (PARTITION BY merchant_category ORDER BY transaction_date) AS cumulative_amount\n",
    "FROM silver.transactions_silver\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15932715-3919-49d0-83a9-b207b93c8b54",
     "showTitle": true,
     "title": "2.2 Calculate the total number of transactions for each combination of transaction_status and merchant_category in transactions_silver."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+------------------+\n|transaction_status|merchant_category|total_transactions|\n+------------------+-----------------+------------------+\n|         completed|           travel|            250148|\n|            failed|           travel|            249848|\n|         completed|         clothing|            249892|\n|            failed|         clothing|            250112|\n+------------------+-----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2.2 Calculate the total number of transactions for each combination of transaction_status and merchant_category in transactions_silver. \n",
    "spark.sql(\"\"\"\n",
    "SELECT transaction_status, merchant_category, COUNT(*) AS total_transactions\n",
    "FROM silver.transactions_silver\n",
    "GROUP BY transaction_status, merchant_category\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72aad945-407f-4c2f-a12a-5cbad4d0ddde",
     "showTitle": true,
     "title": "2.3 Calculate Aggregates (Join based on Customer_ID)-"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----------------+\n|customer_id|week|transaction_count|\n+-----------+----+-----------------+\n| cust_05153|  35|                6|\n| cust_23289|  29|                6|\n| cust_26569|  15|                6|\n| cust_23438|  33|                6|\n| cust_35219|  50|                6|\n| cust_45315|  27|                6|\n| cust_38698|  48|                6|\n| cust_44721|  19|                6|\n| cust_23480|   3|                5|\n| cust_32450|  22|                5|\n+-----------+----+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2.3 Calculate Aggregates (Join based on Customer_ID)\n",
    "# 2.3.1 Select top 10 customers who have done maximum transactions per week\n",
    "spark.sql(\"\"\"\n",
    "SELECT customer_id, weekofyear(transaction_date) AS week, COUNT(*) AS transaction_count\n",
    "FROM silver.transactions_silver\n",
    "GROUP BY customer_id, week\n",
    "ORDER BY transaction_count DESC\n",
    "LIMIT 10\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21236dc6-da41-4aa2-8904-15fc47989477",
     "showTitle": true,
     "title": "2.3.2 Calculate the percentage contribution of each segment to the total monthly transactions."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----------------------+\n|month|total_transactions|percentage_contribution|\n+-----+------------------+-----------------------+\n|   12|             84816|                 8.4816|\n|    1|             85504|                 8.5504|\n|    6|             82080|                  8.208|\n|    3|             84816|                 8.4816|\n|    5|             84816|                 8.4816|\n|    9|             82080|                  8.208|\n|    4|             82080|                  8.208|\n|    8|             84816|                 8.4816|\n|    7|             84816|                 8.4816|\n|   10|             84816|                 8.4816|\n|   11|             82080|                  8.208|\n|    2|             77280|                  7.728|\n+-----+------------------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2.3.2 Calculate the percentage contribution of each segment to the total monthly transactions.\n",
    "\n",
    "monthly_transactions = spark.sql(\"\"\"\n",
    "SELECT month(transaction_date) AS month, COUNT(*) AS total_transactions\n",
    "FROM silver.transactions_silver\n",
    "GROUP BY month\n",
    "\"\"\")\n",
    "\n",
    "total_transactions = monthly_transactions.groupBy().sum(\"total_transactions\").collect()[0][0]\n",
    "\n",
    "monthly_transactions.withColumn(\"percentage_contribution\", (monthly_transactions[\"total_transactions\"] / total_transactions) * 100).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b0a503-29ed-4b92-947b-309e67e71612",
     "showTitle": true,
     "title": "2.3.2 Calculate the percentage contribution of each segment to the total monthly transactions."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+------------------+------------+\n|customer_id|month|    monthly_amount|spender_type|\n+-----------+-----+------------------+------------+\n| cust_18873|    9|           8023.45|        High|\n| cust_24571|    4|          13564.42|        High|\n| cust_31309|    2|12019.619999999999|        High|\n| cust_41750|    2|8025.7300000000005|        High|\n| cust_18867|    6|            6698.1|        High|\n| cust_04378|    2|12922.019999999999|        High|\n| cust_37763|    7|           4499.63|         Low|\n| cust_16545|   12|2313.5099999999998|         Low|\n| cust_09946|    5| 5119.219999999999|        High|\n| cust_01851|   11|           4015.53|         Low|\n| cust_29332|   12|            395.64|         Low|\n| cust_42247|    4|           2879.36|         Low|\n| cust_45743|    8|           9691.91|        High|\n| cust_42057|    3|           3550.65|         Low|\n| cust_24966|   11| 4833.150000000001|         Low|\n| cust_06378|   12|           3962.12|         Low|\n| cust_12620|    4|          15896.04|        High|\n| cust_41696|    5|           9628.39|        High|\n| cust_31501|   10|           6835.69|        High|\n| cust_35940|   11|           8701.23|        High|\n+-----------+-----+------------------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 2.3.2 Calculate the percentage contribution of each segment to the total monthly transactions. \n",
    "from pyspark.sql.functions import when,col,month\n",
    "monthly_spend = spark.sql(\"\"\"\n",
    "SELECT customer_id, month(transaction_date) AS month, SUM(transaction_amount) AS monthly_amount\n",
    "FROM silver.transactions_silver\n",
    "GROUP BY customer_id, month\n",
    "\"\"\")\n",
    "\n",
    "monthly_spend.withColumn(\"spender_type\", when(monthly_spend[\"monthly_amount\"] > 5000, \"High\").otherwise(\"Low\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d791563-f1ee-4eb8-abf2-1b9bda652cf6",
     "showTitle": true,
     "title": "2.3.3 Segment customers into high and low spenders based on monthly transactions."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+------------------+------------+\n|customer_id|month|    monthly_amount|spender_type|\n+-----------+-----+------------------+------------+\n| cust_18873|    9|           8023.45|        High|\n| cust_24571|    4|          13564.42|        High|\n| cust_31309|    2|12019.619999999999|        High|\n| cust_41750|    2|8025.7300000000005|        High|\n| cust_18867|    6|            6698.1|        High|\n| cust_04378|    2|12922.019999999999|        High|\n| cust_37763|    7|           4499.63|         Low|\n| cust_16545|   12|2313.5099999999998|         Low|\n| cust_09946|    5| 5119.219999999999|        High|\n| cust_01851|   11|           4015.53|         Low|\n| cust_29332|   12|            395.64|         Low|\n| cust_42247|    4|           2879.36|         Low|\n| cust_45743|    8|           9691.91|        High|\n| cust_42057|    3|           3550.65|         Low|\n| cust_24966|   11| 4833.150000000001|         Low|\n| cust_06378|   12|           3962.12|         Low|\n| cust_12620|    4|          15896.04|        High|\n| cust_41696|    5|           9628.39|        High|\n| cust_31501|   10|           6835.69|        High|\n| cust_35940|   11|           8701.23|        High|\n+-----------+-----+------------------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 2.3.3 Segment customers into high and low spenders based on monthly transactions.\n",
    "monthly_spend = spark.sql(\"\"\"\n",
    "SELECT customer_id, month(transaction_date) AS month, SUM(transaction_amount) AS monthly_amount\n",
    "FROM silver.transactions_silver\n",
    "GROUP BY customer_id, month\n",
    "\"\"\")\n",
    "\n",
    "monthly_spend.withColumn(\"spender_type\", when(monthly_spend[\"monthly_amount\"] > 5000, \"High\").otherwise(\"Low\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8e79016-2295-480b-bed0-229c0df25f5c",
     "showTitle": true,
     "title": "2.3.4 Rank job categories based on the total transaction_amount in the joined dataset.   Section C: Gold T"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n|         job|        total_amount|\n+------------+--------------------+\n|    services|4.3250783742000055E8|\n|  management| 4.278125024100009E8|\n|     student| 4.267906247700003E8|\n| blue-collar| 4.259000806900003E8|\n|entrepreneur|4.2264909784999996E8|\n|  technician|4.1520509505000085E8|\n+------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2.3.4 Rank job categories based on the total transaction_amount in the joined dataset.\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT c.job, SUM(t.transaction_amount) AS total_amount\n",
    "FROM silver.transactions_silver t\n",
    "JOIN silver.customers_silver c ON t.customer_id = c.customer_id\n",
    "GROUP BY c.job\n",
    "ORDER BY total_amount DESC\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6af7ac4a-8a68-4390-9be8-a7e2dc9a3ecc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Section C: Gold Tables and Final Aggregations \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70d5d02d-c564-4e6f-9bc4-7dee1aa1e272",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------------------------------+\n|count(DISTINCT transaction_type)|count(DISTINCT merchant_category)|\n+--------------------------------+---------------------------------+\n|                               2|                                2|\n+--------------------------------+---------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select count(distinct transaction_type),count(distinct merchant_category ) from bronze.transactions_bronze\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87ad8a7d-1e8b-4a39-9b8c-fefef76cb2c0",
     "showTitle": true,
     "title": "Create Fact/Dim Tables on Gold layer:"
    }
   },
   "outputs": [],
   "source": [
    "# Create Fact/Dim Tables on Gold layer: \n",
    "# Task:  \n",
    "# Create Fact and Dimensions tables via Silver tables on Gold Schema and apply partitioning on respective column. \n",
    "\n",
    "# Create Fact table\n",
    "transactions_silver_df.write.partitionBy(\"transaction_type\").mode(\"overwrite\").format(\"delta\").save(\"/mnt/dataset/transactions_fact_delta\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE gold.transactions_fact\n",
    "    USING DELTA\n",
    "    LOCATION '/mnt/dataset/transactions_fact_delta'\n",
    "\"\"\")\n",
    "\n",
    "# Create Dimension table\n",
    "customers_silver_df.write.bucketBy(8, \"customer_id\").sortBy(\"customer_id\").mode(\"overwrite\").format(\"parquet\").saveAsTable(\"gold.customers_dim\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e52afaa3-585b-4cd2-908a-44474a535eeb",
     "showTitle": true,
     "title": "Create view named Customer_transactions_vw based on Joining Fact/Dimension tables containing functional and attribute columns from both the tables."
    }
   },
   "outputs": [],
   "source": [
    "# Create view named Customer_transactions_vw based on Joining Fact/Dimension tables containing functional and attribute columns from both the tables. \n",
    "customer_transactions_vw = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    t.*, \n",
    "    c.job, \n",
    "    c.balance\n",
    "FROM gold.transactions_fact t\n",
    "JOIN gold.customers_dim c\n",
    "ON t.customer_id = c.customer_id\n",
    "\"\"\")\n",
    "customer_transactions_vw.createOrReplaceTempView(\"Customer_transactions_vw\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e79e4f8-f88c-47d0-a762-e7987c3a723d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2acb969c-fbcf-4263-a66c-a5feea5ae0df",
     "showTitle": true,
     "title": "3.1 Calculate the total balance and the number of customers in each job category (Based on View)."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-------------+\n|         job|       total_balance|num_customers|\n+------------+--------------------+-------------+\n|  management|1.5897315279700124E9|       167591|\n|     student|1.5878093077100046E9|       167365|\n| blue-collar|1.5801121299199913E9|       166910|\n|entrepreneur|1.5882998554299977E9|       165466|\n|  technician|1.5523407036499972E9|       163183|\n|    services|1.6095491742800062E9|       169485|\n+------------+--------------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#3.1 Calculate the total balance and the number of customers in each job category (Based on View). \n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    job, \n",
    "    SUM(balance) AS total_balance, \n",
    "    COUNT(customer_id) AS num_customers\n",
    "FROM Customer_transactions_vw\n",
    "GROUP BY job\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3f9795e-8e7c-471f-afc6-98d19beb36e9",
     "showTitle": true,
     "title": "3.2 Create Visual Chart for Total transactions done based on Merchant Category, Job_type."
    }
   },
   "outputs": [],
   "source": [
    "# 3.2 Create Visual Chart for Total transactions done based on Merchant Category, Job_type. \n",
    "visual_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    job, \n",
    "    merchant_category, \n",
    "    COUNT(transaction_id) AS total_transactions\n",
    "FROM Customer_transactions_vw\n",
    "GROUP BY job, merchant_category\n",
    "\"\"\")\n",
    "# Use Databricks visualization tools to create the chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f006858f-6fd9-4b33-9aa6-e1ec0f5207f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n|avg_purchase_amount|\n+-------------------+\n|  2550.024903197417|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3.3 Derive logic for -  \n",
    "# 3.3.1 Considering only the customers who made purchases, what was the average purchase amount per customer. \n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    AVG(transaction_amount) AS avg_purchase_amount\n",
    "FROM gold.transactions_fact\n",
    "WHERE transaction_type = 'purchase'\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e04879c8-cb4a-46bc-848f-be536e69576d",
     "showTitle": true,
     "title": "3.3.2 Which customer had the highest total transaction amount (sum of purchases minus sum of refunds) on Weekly basis, and what was that amount?"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------------------+\n|customer_id|week|        net_amount|\n+-----------+----+------------------+\n| cust_28869|  36|15569.910000000002|\n+-----------+----+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3.3.2 Which customer had the highest total transaction amount (sum of purchases minus sum of refunds) on Weekly basis, and what was that amount? \n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    customer_id, \n",
    "    weekofyear(transaction_date) AS week, \n",
    "    (SUM(CASE WHEN transaction_type = 'purchase' THEN transaction_amount ELSE 0 END) - \n",
    "    SUM(CASE WHEN transaction_type = 'refund' THEN transaction_amount ELSE 0 END)) AS net_amount\n",
    "FROM gold.transactions_fact\n",
    "GROUP BY customer_id, week\n",
    "ORDER BY net_amount DESC\n",
    "LIMIT 1\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd789a2d-c8c6-4865-8fc1-b8bf2421d32c",
     "showTitle": true,
     "title": "3.4 Build logic for Reconcile the total number of records in the bronze, silver, and gold tables for customer and transactions datasets."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-------+\n| layer|       table|  count|\n+------+------------+-------+\n|bronze|transactions|1000000|\n|bronze|   customers|1000000|\n|  gold|   customers|1000000|\n|silver|transactions|1000000|\n|silver|   customers|1000000|\n|  gold|transactions|1000000|\n+------+------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3.4 Build logic for Reconcile the total number of records in the bronze, silver, and gold tables for customer and transactions datasets.  \n",
    "bronze_counts = spark.sql(\"\"\"\n",
    "SELECT 'bronze' AS layer, 'transactions' AS table, COUNT(*) AS count FROM bronze.transactions_bronze\n",
    "UNION ALL\n",
    "SELECT 'bronze', 'customers', COUNT(*) FROM bronze.customer_bronze\n",
    "\"\"\")\n",
    "\n",
    "silver_counts = spark.sql(\"\"\"\n",
    "SELECT 'silver' AS layer, 'transactions' AS table, COUNT(*) AS count FROM silver.transactions_silver\n",
    "UNION ALL\n",
    "SELECT 'silver', 'customers', COUNT(*) FROM silver.customers_silver\n",
    "\"\"\")\n",
    "\n",
    "gold_counts = spark.sql(\"\"\"\n",
    "SELECT 'gold' AS layer, 'transactions' AS table, COUNT(*) AS count FROM gold.transactions_fact\n",
    "UNION ALL\n",
    "SELECT 'gold', 'customers', COUNT(*) FROM gold.customers_dim\n",
    "\"\"\")\n",
    "\n",
    "bronze_counts.union(silver_counts).union(gold_counts).show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Celebal Case Study",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
